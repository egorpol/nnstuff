{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neuron Playground: Interactive intuition + backprop mini‑lab\n",
    "\n",
    "This notebook complements `neuron.ipynb` with hands‑on, interactive visuals:\n",
    "\n",
    "- Activation explorer (step, sigmoid, tanh, ReLU, leaky ReLU, softplus, GELU) with derivative overlay\n",
    "- Single 2D neuron decision boundary (weights/bias sliders) with probability heatmap\n",
    "- Weighted sum playground (tweak inputs, weights, bias; see activations)\n",
    "- Backprop mini‑lab (single neuron, MSE): gradients, one‑step updates, and contour view of L(w1, w2)\n",
    "\n",
    "Tip: Run the cells from top to bottom. If a dependency is missing, the setup cell below can install it in‑notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup (run once)\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def _ensure(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except Exception:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "\n",
    "# ipywidgets for interactivity\n",
    "_ensure('ipywidgets')\n",
    "_ensure('matplotlib')\n",
    "_ensure('numpy')\n",
    "_ensure('scipy')\n",
    "_ensure('pandas')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, FloatSlider, IntSlider, Dropdown, Checkbox\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "plt.rcParams['figure.dpi'] = 120\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation explorer: why derivatives and ranges?\n",
    "\n",
    "- **What is a derivative (in plain words)?** The derivative is the curve’s local slope at a point. It tells you how sensitive the output is to a tiny change in the input right there.\n",
    "  - Positive slope → increase input a little, output goes up.\n",
    "  - Negative slope → increase input a little, output goes down.\n",
    "  - Near zero slope → the curve is flat there; small input changes barely affect the output.\n",
    "  - Bigger magnitude → more sensitivity (a small input nudge causes a larger output nudge).\n",
    "- **What is a gradient?** When something depends on many variables (e.g., many weights), the gradient is the collection of all those slopes—one per variable. It points toward the direction of fastest increase. Learning uses the opposite direction (downhill) to reduce error.\n",
    "- **In this plot:** The dashed line shows the derivative of the activation with respect to its input z (the neuron’s weighted sum). It answers: “If z changes a tiny bit, how much does the activation change?”\n",
    "- **Why this matters for learning/backprop:** Backprop chains these slopes through layers. If slopes are very small in many places, signals shrink (vanishing gradients) and learning becomes slow. If slopes are very large, updates can be unstable (exploding gradients).\n",
    "\n",
    "- **Examples of derivative behavior across activations**\n",
    "  - **sigmoid**: derivative ≤ 0.25 and goes to 0 for large |x| (vanishing gradients in saturation).\n",
    "  - **tanh**: similar shape but centered at 0; derivative peaks at 1 and also vanishes in saturation.\n",
    "  - **ReLU**: derivative is 0 for x<0 and 1 for x>0 (sparse gradients; no saturation on the positive side).\n",
    "  - **leaky ReLU / softplus / GELU**: smooth or non‑zero negative‑side slopes to improve gradient flow.\n",
    "- **Why these min/max sliders?** In practice, inputs or pre‑activations z often fall roughly in [−6, 6] for normalized data and well‑scaled weights. That range lets you see both linear and saturated regions for sigmoid/tanh, and enough negative/positive range for ReLU variants. You can widen it to explore edge behavior.\n",
    "- **Common choices today**\n",
    "  - **Hidden layers**: ReLU is a strong default; **Leaky ReLU** (α≈0.01–0.2), **GELU** or **SiLU/Swish** are common in modern architectures.\n",
    "  - **Output layers**: sigmoid (binary), softmax (multiclass), tanh (bounded symmetric outputs), identity/linear (regression).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "236349d26ad24798b54ebdd7a6a87c63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='activation', index=1, options=('step', 'sigmoid', 'tanh', 'relu', …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Activation explorer with derivative overlay\n",
    "\n",
    "def step(x):\n",
    "\treturn (x > 0).astype(float)\n",
    "\n",
    "def sigmoid(x):\n",
    "\treturn 1.0 / (1.0 + np.exp(-x))\n",
    "\n",
    "def tanh(x):\n",
    "\treturn np.tanh(x)\n",
    "\n",
    "def relu(x):\n",
    "\treturn np.maximum(0.0, x)\n",
    "\n",
    "def leaky_relu(x, alpha=0.01):\n",
    "\treturn np.where(x > 0, x, alpha * x)\n",
    "\n",
    "def softplus(x):\n",
    "\t# stable softplus\n",
    "\treturn np.log1p(np.exp(-np.abs(x))) + np.maximum(x, 0)\n",
    "\n",
    "def gelu(x):\n",
    "\t# tanh approximation\n",
    "\treturn 0.5 * x * (1 + np.tanh(np.sqrt(2/np.pi) * (x + 0.044715 * x**3)))\n",
    "\n",
    "# Derivatives\n",
    "\n",
    "def d_step(x):\n",
    "\treturn np.zeros_like(x)  # undefined at 0; show 0 for visualization\n",
    "\n",
    "def d_sigmoid(x):\n",
    "\ts = sigmoid(x)\n",
    "\treturn s * (s - 1) * -1  # s*(1-s)\n",
    "\n",
    "def d_tanh(x):\n",
    "\tt = tanh(x)\n",
    "\treturn 1 - t**2\n",
    "\n",
    "def d_relu(x):\n",
    "\treturn (x > 0).astype(float)\n",
    "\n",
    "def d_leaky_relu(x, alpha=0.01):\n",
    "\tg = np.ones_like(x)\n",
    "\tg[x < 0] = alpha\n",
    "\tg[x == 0] = alpha  # arbitrary\n",
    "\treturn g\n",
    "\n",
    "def d_softplus(x):\n",
    "\treturn sigmoid(x)\n",
    "\n",
    "def d_gelu(x):\n",
    "\t# derivative of tanh approximation\n",
    "\tc = np.sqrt(2/np.pi)\n",
    "\tt = np.tanh(c * (x + 0.044715 * x**3))\n",
    "\tsech2 = 1 - t**2\n",
    "\tinner = c * (1 + 0.134145 * x**2)\n",
    "\treturn 0.5 * (1 + t) + 0.5 * x * sech2 * inner\n",
    "\n",
    "_act = {\n",
    "\t\"step\": (step, d_step),\n",
    "\t\"sigmoid\": (sigmoid, d_sigmoid),\n",
    "\t\"tanh\": (tanh, d_tanh),\n",
    "\t\"relu\": (relu, d_relu),\n",
    "\t\"leaky_relu\": (leaky_relu, d_leaky_relu),\n",
    "\t\"softplus\": (softplus, d_softplus),\n",
    "\t\"gelu\": (gelu, d_gelu),\n",
    "}\n",
    "\n",
    "@interact(\n",
    "\tname=Dropdown(options=list(_act.keys()), value='sigmoid', description='activation'),\n",
    "\txmin=FloatSlider(-6, min=-10, max=0, step=0.5, description='xmin'),\n",
    "\txmax=FloatSlider(6, min=0, max=10, step=0.5, description='xmax'),\n",
    "\tshow_deriv=Checkbox(True, description='show derivative'),\n",
    ")\n",
    "def _activation_explorer(name, xmin, xmax, show_deriv):\n",
    "\tx = np.linspace(xmin, xmax, 800)\n",
    "\tf, df = _act[name]\n",
    "\tif name == 'leaky_relu':\n",
    "\t\ty = f(x, 0.02)\n",
    "\t\tdy = df(x, 0.02)\n",
    "\telse:\n",
    "\t\ty = f(x)\n",
    "\t\tdy = df(x)\n",
    "\tplt.figure(figsize=(6,3.6))\n",
    "\tplt.plot(x, y, label=name)\n",
    "\tif show_deriv:\n",
    "\t\tplt.plot(x, dy, '--', label=f'd{name}')\n",
    "\tplt.axhline(0, color='k', linewidth=0.5)\n",
    "\tplt.axvline(0, color='k', linewidth=0.5)\n",
    "\tplt.grid(True, linestyle='--', alpha=0.5)\n",
    "\tplt.legend()\n",
    "\tplt.title('Activation and derivative')\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted sum playground: how inputs, weights, and bias form z\n",
    "\n",
    "- **Computation:** z = i1·w1 + i2·w2 + b. The activation a = f(z) then produces the neuron’s output.\n",
    "- **Visual schema:** See the diagram in the next cell (inputs → weighted sum + bias → activation → output).\n",
    "- **During learning/backprop (non-technical):**\n",
    "  - We measure an error (loss) between the output a and the target y.\n",
    "  - We compute sensitivities (gradients): how changing each weight or the bias would change that loss.\n",
    "  - We nudge weights and bias a little in the downhill direction to reduce the loss.\n",
    "- **Can weights be negative?** Yes. A negative weight means the input inhibits the neuron (increases input lowers z). Positive weights excite the neuron (increases input raises z). Bias shifts the threshold left/right.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 900x336 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visual schema: inputs, weighted sum, bias, activation, output\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import FancyBboxPatch, Circle, FancyArrowPatch, Patch\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(7.5, 2.8))\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.axis('off')\n",
    "\n",
    "# Helpers\n",
    "\n",
    "def draw_box(ax, x, y, w, h, text, facecolor='#f0f4ff', edgecolor='#3b6ea8'):\n",
    "\tbox = FancyBboxPatch((x, y), w, h, boxstyle='round,pad=0.02,rounding_size=0.03',\n",
    "\t\t\t\t\t\t fc=facecolor, ec=edgecolor, lw=1.6)\n",
    "\tax.add_patch(box)\n",
    "\tax.text(x + w/2, y + h/2, text, ha='center', va='center', fontsize=10)\n",
    "\treturn box\n",
    "\n",
    "def draw_circle(ax, x, y, r, text=None, facecolor='#ffffff', edgecolor='#555555'):\n",
    "\tcirc = Circle((x, y), r, fc=facecolor, ec=edgecolor, lw=1.6)\n",
    "\tax.add_patch(circ)\n",
    "\tif text is not None:\n",
    "\t\tax.text(x, y, text, ha='center', va='center', fontsize=10)\n",
    "\treturn circ\n",
    "\n",
    "def draw_arrow(ax, start, end, text=None):\n",
    "\tarrow = FancyArrowPatch(start, end, arrowstyle='->', mutation_scale=12, lw=1.6, color='#333333')\n",
    "\tax.add_patch(arrow)\n",
    "\tif text is not None:\n",
    "\t\tx = (start[0] + end[0]) / 2\n",
    "\t\ty = (start[1] + end[1]) / 2\n",
    "\t\tax.text(x, y + 0.05, text, ha='center', va='center', fontsize=9, color='#333333')\n",
    "\treturn arrow\n",
    "\n",
    "# Nodes\n",
    "i1 = draw_circle(ax, 0.08, 0.72, 0.04, text='i1')\n",
    "i2 = draw_circle(ax, 0.08, 0.28, 0.04, text='i2')\n",
    "\n",
    "sum_box = draw_box(ax, 0.24, 0.20, 0.22, 0.60, 'weighted\\nsum', facecolor='#eef7ff')\n",
    "\n",
    "act_box = draw_box(ax, 0.58, 0.34, 0.20, 0.32, 'activation\\nϕ(z)', facecolor='#effaf0', edgecolor='#2f7d31')\n",
    "\n",
    "out = draw_circle(ax, 0.88, 0.50, 0.04, text='a')\n",
    "\n",
    "# Connections\n",
    "draw_arrow(ax, (0.12, 0.72), (0.24, 0.62), text='w1')\n",
    "draw_arrow(ax, (0.12, 0.28), (0.24, 0.38), text='w2')\n",
    "\n",
    "# z arrow and +b hint\n",
    "draw_arrow(ax, (0.46, 0.50), (0.58, 0.50), text='z')\n",
    "ax.text(0.52, 0.62, '+ b', ha='center', va='center', fontsize=10)\n",
    "\n",
    "# activation to output\n",
    "draw_arrow(ax, (0.78, 0.50), (0.84, 0.50))\n",
    "\n",
    "# Legend\n",
    "legend_handles = [\n",
    "\tPatch(facecolor='#eef7ff', edgecolor='#3b6ea8', label='weighted sum'),\n",
    "\tPatch(facecolor='#effaf0', edgecolor='#2f7d31', label='activation ϕ(z)'),\n",
    "\tPatch(facecolor='#ffffff', edgecolor='#555555', label='inputs/output')\n",
    "]\n",
    "ax.legend(handles=legend_handles, loc='upper center', bbox_to_anchor=(0.5, 1.05),\n",
    "\t\t   ncol=3, frameon=False, fontsize=9)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.92])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f3ccfc4f5d4de6b53e7001e3b8a49e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='i1', max=5.0, min=-5.0), FloatSlider(value=0.5, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Weighted sum playground: inputs, weights, bias, activation\n",
    "\n",
    "@interact(\n",
    "\ti1=FloatSlider(1.0, min=-5, max=5, step=0.1, description='i1'),\n",
    "\ti2=FloatSlider(0.5, min=-5, max=5, step=0.1, description='i2'),\n",
    "\tw1=FloatSlider(1.0, min=-5, max=5, step=0.1, description='w1'),\n",
    "\tw2=FloatSlider(-0.5, min=-5, max=5, step=0.1, description='w2'),\n",
    "\tb=FloatSlider(0.0, min=-5, max=5, step=0.1, description='bias'),\n",
    "\tactivation=Dropdown(options=['sigmoid','tanh','relu','softplus','step'], value='sigmoid'),\n",
    ")\n",
    "def _weighted_sum_playground(i1, i2, w1, w2, b, activation):\n",
    "\tinputs = np.array([i1, i2])\n",
    "\tweights = np.array([w1, w2])\n",
    "\tz = np.dot(inputs, weights) + b\n",
    "\tif activation == 'sigmoid':\n",
    "\t\ta = sigmoid(z)\n",
    "\telif activation == 'tanh':\n",
    "\t\ta = tanh(z)\n",
    "\telif activation == 'relu':\n",
    "\t\ta = relu(z)\n",
    "\telif activation == 'softplus':\n",
    "\t\ta = softplus(z)\n",
    "\telse:\n",
    "\t\ta = step(z)\n",
    "\tprint(f\"z = inputs·weights + b = {z:.4f}\")\n",
    "\tprint(f\"a = {activation}(z) = {a:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2D decision boundary: how z and a are shown here (vs previous cell)\n",
    "\n",
    "- Previously, we computed a single weighted sum z and activation a for one pair of inputs (i1, i2).\n",
    "- Here, we evaluate z = w1·x1 + w2·x2 + b on a whole grid of points (x1, x2), then show the activation a for each point as a color.\n",
    "- Heatmap colors = activation a(x1, x2):\n",
    "  - For sigmoid: this can be read as probability p(y=1|x).\n",
    "  - For tanh: we plot (tanh(z) + 1) / 2 so the colors fit 0–1.\n",
    "  - For ReLU/softplus: we show a clipped/normalized view for readability (not a true probability).\n",
    "- The white line labeled z=0 is the decision boundary (the set of points where the weighted sum changes sign). For sigmoid, it roughly corresponds to p ≈ 0.5 on that line.\n",
    "- Sliders control w1, w2, b:\n",
    "  - w1, w2 change the orientation/slope of the boundary.\n",
    "  - b shifts the boundary left/right or up/down.\n",
    "- Use the x/y range sliders to zoom the view.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb3e57ea643e4c39bf81447555d71258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='w1', max=5.0, min=-5.0), FloatSlider(value=1.0, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2D decision boundary explorer with probability heatmap\n",
    "\n",
    "@interact(\n",
    "\tw1=FloatSlider(1.0, min=-5, max=5, step=0.1, description='w1'),\n",
    "\tw2=FloatSlider(1.0, min=-5, max=5, step=0.1, description='w2'),\n",
    "\tb=FloatSlider(0.0, min=-5, max=5, step=0.1, description='bias'),\n",
    "\tactivation=Dropdown(options=['sigmoid','tanh','relu','softplus','step'], value='sigmoid'),\n",
    "\txmin=FloatSlider(-3, min=-5, max=0, step=0.5, description='xmin'),\n",
    "\txmax=FloatSlider(3, min=0, max=5, step=0.5, description='xmax'),\n",
    "\tymin=FloatSlider(-3, min=-5, max=0, step=0.5, description='ymin'),\n",
    "\tymax=FloatSlider(3, min=0, max=5, step=0.5, description='ymax'),\n",
    ")\n",
    "def _decision_boundary(w1, w2, b, activation, xmin, xmax, ymin, ymax):\n",
    "\txx, yy = np.meshgrid(\n",
    "\t\tnp.linspace(xmin, xmax, 200),\n",
    "\t\tnp.linspace(ymin, ymax, 200),\n",
    "\t)\n",
    "\tz = w1 * xx + w2 * yy + b\n",
    "\tif activation == 'sigmoid':\n",
    "\t\tp = sigmoid(z)\n",
    "\telif activation == 'tanh':\n",
    "\t\tp = (tanh(z) + 1) / 2\n",
    "\telif activation == 'relu':\n",
    "\t\tp = np.clip(relu(z) / (1 + np.abs(z)), 0, 1)\n",
    "\telif activation == 'softplus':\n",
    "\t\tp = np.clip(softplus(z) / (1 + np.abs(z)), 0, 1)\n",
    "\telse:\n",
    "\t\tp = step(z)\n",
    "\tplt.figure(figsize=(6,5))\n",
    "\tcf = plt.contourf(xx, yy, p, levels=50, cmap='viridis')\n",
    "\tcs = plt.contour(xx, yy, z, levels=[0.0], colors='white', linewidths=2)\n",
    "\tplt.clabel(cs, fmt={0.0: 'z=0'}, inline=True)\n",
    "\tplt.colorbar(cf, label='activation/probability')\n",
    "\tplt.title('Neuron decision boundary and activation heatmap')\n",
    "\tplt.xlabel('x1')\n",
    "\tplt.ylabel('x2')\n",
    "\tplt.grid(alpha=0.2)\n",
    "\tplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop mini‑lab (single neuron, MSE)\n",
    "\n",
    "- **What is a labeled sample?** One training example with inputs and the desired answer: `(x1, x2, y)`.\n",
    "  - `x1, x2`: the features fed into the neuron\n",
    "  - `y`: the target output we want (for sigmoid, usually 0 or 1)\n",
    "\n",
    "- **Connection to previous cells**\n",
    "  - Earlier, we only did the forward pass: compute `z = w1·x1 + w2·x2 + b` and `a = ϕ(z)` for a chosen input. The heatmap showed `a` over many inputs `(x1, x2)` and drew the `z = 0` boundary in input space.\n",
    "  - Here, we fix a single input `(x1, x2)` and ask a different question: “How should we change the parameters `(w1, w2, b)` so the neuron’s output `a` gets closer to the target `y`?” This is the learning step.\n",
    "\n",
    "- **What this cell does**\n",
    "  - Computes the current `z`, activation `a`, and the error (“loss”) for your chosen `(x1, x2, y)`.\n",
    "  - Computes sensitivities (gradients) telling us how much the loss would change if we nudged each parameter (`w1`, `w2`, `b`) a tiny bit.\n",
    "  - Takes one small downhill step (size set by the learning rate `lr`) to reduce the loss, and reports the new parameters and new loss.\n",
    "  - Optionally plots contours of the loss `L(w1, w2)` while keeping `b` fixed, so you can see where “downhill” lies in parameter space. You’ll see:\n",
    "    - Cyan point: current `(w1, w2)`\n",
    "    - Yellow point: after one step\n",
    "    - Green arrow: update direction\n",
    "\n",
    "- **Learning rate slider (`lr`)**\n",
    "  - Sets the step size for each update. Smaller `lr` → safer but slower; larger `lr` → faster but can overshoot and increase the loss.\n",
    "  - A practical way to feel it: start around 0.05–0.3, try larger; if loss jumps up after a step or oscillates, reduce `lr`.\n",
    "\n",
    "- **Different views, different spaces**\n",
    "  - Heatmap cell: input space `(x1, x2)` → color shows output `a`.\n",
    "  - This mini‑lab: parameter space `(w1, w2)` → contours show loss `L` for your fixed input/target. Numbers on contours are the actual loss values.\n",
    "\n",
    "- **Loss formula (plain words)**\n",
    "  - `L = 1/2 · (a − y)^2` measures how far the output `a` is from the target `y`; the 1/2 is a convenience and doesn’t change what is “better/worse”.\n",
    "  - Example (y=1): if `a=0.6` then `L ≈ 0.08`; if `a=0.4` then `L ≈ 0.18` (closer to 1 is better, so smaller loss).\n",
    "  - Example (y=0): if `a=0.2` then `L ≈ 0.02`; if `a=0.8` then `L ≈ 0.32` (closer to 0 is better).\n",
    "\n",
    "- **Why use MSE here instead of cross‑entropy?**\n",
    "  - Simplicity for a single example: easy to read and visualize as smooth contours.\n",
    "  - For classification with sigmoid outputs, cross‑entropy is usually preferred because it gives stronger learning signals when the model is confidently wrong and aligns with probabilistic modeling (maximum likelihood).\n",
    "  - You’ll see cross‑entropy in the multi‑sample demo below; the learning mechanics are the same (loss → gradients → step), only the loss shape differs.\n",
    "\n",
    "- **What are the gradients `dL/dw1`, `dL/dw2`, `dL/db`?**\n",
    "  - They are the slopes of the loss with respect to each parameter: how much `L` changes if you nudge that parameter a tiny bit.\n",
    "  - Intuition via dependencies:\n",
    "    - `z = w1·x1 + w2·x2 + b` → changing `w1` by a small amount changes `z` by about `x1` times that amount. So `dL/dw1 = (dL/dz) · x1`.\n",
    "    - Similarly, `dL/dw2 = (dL/dz) · x2` and `dL/db = dL/dz` (bias shifts `z` directly).\n",
    "    - For MSE with activation `a = ϕ(z)`, we have `dL/dz = (a − y) · ϕ’(z)` (loss mismatch times activation slope).\n",
    "  - The printed gradients tell you the direction to change each parameter to reduce `L` (we move a small step in the negative gradient direction).\n",
    "  - Notation: `d` means a tiny change/derivative. Read `dL` as “a tiny change in loss,” and `dL/dw1` as “slope of `L` with respect to `w1`.” Likewise `ϕ’(z)` is the derivative (slope) of the activation at `z`. \n",
    "\n",
    "- **Why activation slopes matter**\n",
    "  - The gradients include the activation’s local slope (its derivative). In flat/saturated regions, slopes are tiny → tiny gradients → tiny updates (slow learning). In steeper regions, updates are larger.\n",
    "\n",
    "Loss: `L = 1/2 · (a − y)^2` where `a = ϕ(z)` and `z = w1·x1 + w2·x2 + b`. For classification with sigmoid, cross‑entropy is common (used later), but the learning idea—compute loss, compute gradients, step downhill—is the same.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are loss contours? What do the numbers mean?\n",
    "\n",
    "- A contour line connects all parameter pairs `(w1, w2)` that give the same loss value `L` for the fixed input `(x1, x2)`, target `y`, and bias `b`.\n",
    "- The numbers printed on the lines are the actual loss values `L` on those lines (lower is better). With MSE and a sigmoid output, `L` ranges from 0 (perfect) up to about 0.5 (very wrong for y=0/1).\n",
    "- Moving perpendicular to the contours changes the loss the fastest (that direction is the negative gradient direction we step toward).\n",
    "- Near a good solution, contours look like nested ovals around a low point; tightly packed lines indicate a steep region; widely spaced lines indicate a flat region.\n",
    "\n",
    "#### Concrete numeric example (sigmoid + MSE)\n",
    "\n",
    "Consider a single labeled sample `(x1, x2, y) = (1.0, 2.0, 1.0)` and bias `b = 0`.\n",
    "We vary `(w1, w2)` to see how `z`, activation `a = sigmoid(z)`, and loss `L = 1/2·(a−y)^2` change:\n",
    "\n",
    "| w1 | w2  | z = w1·x1 + w2·x2 + b | a = sigmoid(z) | y   | L = 1/2·(a−y)^2 |\n",
    "|----|-----|------------------------|----------------|-----|------------------|\n",
    "| 0.5 | −0.5 | −0.5                   | 0.378          | 1.0 | 0.193            |\n",
    "| 1.5 | −0.5 | 0.5                    | 0.622          | 1.0 | 0.071            |\n",
    "| 0.5 | −1.5 | −2.5                   | 0.076          | 1.0 | 0.427            |\n",
    "\n",
    "- Interpretation:\n",
    "  - Moving `w1` up from 0.5 to 1.5 raised `z` and `a`, bringing `a` closer to `y=1.0` and lowering `L` (from ~0.193 to ~0.071).\n",
    "  - Making `w2` more negative at `−1.5` pushed `z` further negative, `a` got smaller, and the loss increased (~0.427).\n",
    "- In the contour plot, each of these `(w1, w2)` points lies on a contour labeled by its loss (e.g., around `0.19`, `0.07`, or `0.43`). The update arrow points roughly toward lower‑valued contours.\n",
    "\n",
    "The cyan/yellow points and green arrow show where you are and where one learning step moves you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a281d0f9d67645b19b204b92948b7757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='x1', max=3.0, min=-3.0), FloatSlider(value=2.0, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _select_activation(name):\n",
    "\tif name == 'sigmoid':\n",
    "\t\treturn sigmoid, d_sigmoid\n",
    "\tif name == 'tanh':\n",
    "\t\treturn tanh, d_tanh\n",
    "\tif name == 'relu':\n",
    "\t\treturn relu, d_relu\n",
    "\tif name == 'softplus':\n",
    "\t\treturn softplus, d_softplus\n",
    "\treturn sigmoid, d_sigmoid\n",
    "\n",
    "@interact(\n",
    "\tx1=FloatSlider(1.0, min=-3, max=3, step=0.1, description='x1'),\n",
    "\tx2=FloatSlider(2.0, min=-3, max=3, step=0.1, description='x2'),\n",
    "\ty=FloatSlider(1.0, min=0, max=1, step=0.05, description='y'),\n",
    "\tw1=FloatSlider(0.5, min=-3, max=3, step=0.05, description='w1'),\n",
    "\tw2=FloatSlider(-0.5, min=-3, max=3, step=0.05, description='w2'),\n",
    "\tb=FloatSlider(0.0, min=-3, max=3, step=0.05, description='b'),\n",
    "\tlr=FloatSlider(0.1, min=0.001, max=1.0, step=0.001, readout_format='.3f', description='lr'),\n",
    "\tactivation=Dropdown(options=['sigmoid','tanh','relu','softplus'], value='sigmoid', description='act'),\n",
    "\tshow_contours=Checkbox(True, description='show contours'),\n",
    ")\n",
    "def _backprop_one_sample(x1, x2, y, w1, w2, b, lr, activation, show_contours):\n",
    "\tf, df = _select_activation(activation)\n",
    "\tz = w1 * x1 + w2 * x2 + b\n",
    "\ta = f(z)\n",
    "\tL = 0.5 * (a - y) ** 2\n",
    "\t# dL/da = (a - y), da/dz = f'(z), dz/dw1 = x1, dz/dw2 = x2, dz/db = 1\n",
    "\tdL_dz = (a - y) * df(np.array([z]))[0]\n",
    "\tdL_dw1 = dL_dz * x1\n",
    "\tdL_dw2 = dL_dz * x2\n",
    "\tdL_db = dL_dz\n",
    "\tprint(f\"z={z:.4f}, a={a:.4f}, L={L:.6f}\")\n",
    "\tprint(f\"gradients: dL/dw1={dL_dw1:.6f}, dL/dw2={dL_dw2:.6f}, dL/db={dL_db:.6f}\")\n",
    "\tw1_new = w1 - lr * dL_dw1\n",
    "\tw2_new = w2 - lr * dL_dw2\n",
    "\tb_new = b - lr * dL_db\n",
    "\tz_new = w1_new * x1 + w2_new * x2 + b_new\n",
    "\ta_new = f(z_new)\n",
    "\tL_new = 0.5 * (a_new - y) ** 2\n",
    "\tprint(f\"after 1 step: w1={w1_new:.4f}, w2={w2_new:.4f}, b={b_new:.4f}, L={L_new:.6f}\")\n",
    "\tif show_contours:\n",
    "\t\t# show L(w1,w2) contours at fixed b\n",
    "\t\tw1s = np.linspace(w1 - 2, w1 + 2, 100)\n",
    "\t\tw2s = np.linspace(w2 - 2, w2 + 2, 100)\n",
    "\t\tWW1, WW2 = np.meshgrid(w1s, w2s)\n",
    "\t\tZ = WW1 * x1 + WW2 * x2 + b\n",
    "\t\tA = f(Z)\n",
    "\t\tLoss = 0.5 * (A - y) ** 2\n",
    "\t\tplt.figure(figsize=(6,5))\n",
    "\t\tcs = plt.contour(WW1, WW2, Loss, levels=20, cmap='magma')\n",
    "\t\tplt.clabel(cs, inline=True, fontsize=8)\n",
    "\t\tplt.scatter([w1], [w2], c='cyan', edgecolors='k', label='current')\n",
    "\t\tplt.scatter([w1_new], [w2_new], c='yellow', edgecolors='k', label='after step')\n",
    "\t\tplt.quiver([w1], [w2], [w1_new - w1], [w2_new - w2], angles='xy', scale_units='xy', scale=1, color='lime', width=0.005, label='update')\n",
    "\t\tplt.xlabel('w1')\n",
    "\t\tplt.ylabel('w2')\n",
    "\t\tplt.title('Loss contours L(w1,w2) at fixed b')\n",
    "\t\tplt.legend()\n",
    "\t\tplt.grid(alpha=0.2)\n",
    "\t\tplt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-sample gradient descent (toy dataset)\n",
    "\n",
    "- **What this builds on**\n",
    "  - From the single‑sample mini‑lab, you saw: loss, gradients, and one step for a single `(x1, x2, y)`.\n",
    "  - From the decision‑boundary heatmap, you saw: how `z = w1·x1 + w2·x2 + b` and activation `a` vary over input space.\n",
    "  - Here, we combine both ideas over a whole dataset and update parameters repeatedly (epochs).\n",
    "\n",
    "- **Data**\n",
    "  - We generate a tiny 2‑class dataset with two clusters (roughly linearly separable). Each point is an input `(x1, x2)` with a label `y ∈ {0,1}`.\n",
    "  - The `seed` slider reshuffles the random draw.\n",
    "  - You can preview the dataset in a table (see the toggle in the cell below).\n",
    "\n",
    "- **Model**\n",
    "  - A single logistic neuron: `z = w1·x1 + w2·x2 + b`, `a = sigmoid(z)` interpreted as `p(y=1|x)`.\n",
    "  - Where it’s used: logistic regression (classic baseline classifier), final layer of binary classifiers, and as a building block inside larger neural networks.\n",
    "\n",
    "- **Loss (cross‑entropy, averaged over samples)**\n",
    "  - `L = −mean( y·log(a) + (1−y)·log(1−a) )`.\n",
    "  - With sigmoid + cross‑entropy, per‑example `dL/dz = a − y` (same chain rule idea as before).\n",
    "\n",
    "- **Updates over epochs**\n",
    "  - Average gradients: `dL/dw = mean( (a−y)·x )`, `dL/db = mean(a−y)`.\n",
    "  - Update: `w ← w − lr·dL/dw`, `b ← b − lr·dL/db`, repeated for `epochs` steps.\n",
    "\n",
    "- **What the plots show**\n",
    "  - Left: data and the learned decision boundary/heatmap after training.\n",
    "    - Blue/orange points are the two classes.\n",
    "    - Black line is `z=0` (decision boundary); colored background is `a = sigmoid(z)` = `p(y=1|x)`.\n",
    "  - Right: training loss (binary cross‑entropy) as it changes over epochs.\n",
    "    - Lower and smoother is better; if it increases or oscillates, try a smaller `lr` or different initialization.\n",
    "    - `epochs` and `lr` most strongly affect this curve: more epochs → more chances to descend; `lr` controls step size.\n",
    "\n",
    "- **Links to earlier cells**\n",
    "  - The `z=0` boundary is the same concept as in the 2D boundary explorer, now positioned by training instead of sliders.\n",
    "  - Gradients/updates are the same idea as the single‑sample backprop lab, now averaged across many samples each epoch.\n",
    "\n",
    "- **Sliders**\n",
    "  - `epochs`: number of training passes.\n",
    "  - `lr`: learning rate (step size). Too large → oscillation/divergence; too small → slow.\n",
    "  - `w1_0`, `w2_0`, `b0`: starting parameters (affect convergence speed and direction).\n",
    "\n",
    "Tip: If the classes overlap, a single straight line cannot perfectly separate them; expect a non‑zero loss at convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7077337a5c86406cb253d54a9b0decfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='seed', max=999), IntSlider(value=20, description='epochs…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def _make_toy_data(n=40, seed=0):\n",
    "\trng = np.random.default_rng(seed)\n",
    "\tx1_pos = rng.normal(loc=1.0, scale=0.6, size=n//2)\n",
    "\tx2_pos = rng.normal(loc=1.0, scale=0.6, size=n//2)\n",
    "\tx1_neg = rng.normal(loc=-1.0, scale=0.6, size=n//2)\n",
    "\tx2_neg = rng.normal(loc=-1.0, scale=0.6, size=n//2)\n",
    "\tX = np.vstack([\n",
    "\t\tnp.stack([x1_pos, x2_pos], axis=1),\n",
    "\t\tnp.stack([x1_neg, x2_neg], axis=1),\n",
    "\t])\n",
    "\ty = np.concatenate([np.ones(n//2), np.zeros(n//2)])\n",
    "\treturn X, y\n",
    "\n",
    "@interact(\n",
    "\tseed=IntSlider(0, min=0, max=999, step=1, description='seed'),\n",
    "\tepochs=IntSlider(20, min=1, max=200, step=1, description='epochs'),\n",
    "\tlr=FloatSlider(0.1, min=0.001, max=1.0, step=0.001, readout_format='.3f', description='lr'),\n",
    "\tb_init=FloatSlider(0.0, min=-3, max=3, step=0.1, description='b0'),\n",
    "\tw1_init=FloatSlider(0.0, min=-2, max=2, step=0.1, description='w1_0'),\n",
    "\tw2_init=FloatSlider(0.0, min=-2, max=2, step=0.1, description='w2_0'),\n",
    "\tshow_data=Checkbox(False, description='show data table'),\n",
    "\t\n",
    ")\n",
    "def _logistic_gd(seed, epochs, lr, b_init, w1_init, w2_init, show_data):\n",
    "\tX, y = _make_toy_data(40, seed)\n",
    "\tif show_data:\n",
    "\t\tdf = pd.DataFrame({'x1': X[:,0], 'x2': X[:,1], 'y': y.astype(int)})\n",
    "\t\tdisplay(df)\n",
    "\tw = np.array([w1_init, w2_init], dtype=float)\n",
    "\tb = float(b_init)\n",
    "\tlosses = []\n",
    "\tfor _ in range(epochs):\n",
    "\t\tz = X @ w + b\n",
    "\t\ta = sigmoid(z)\n",
    "\t\t# binary cross-entropy loss\n",
    "\t\tL = -np.mean(y * np.log(a + 1e-9) + (1 - y) * np.log(1 - a + 1e-9))\n",
    "\t\tlosses.append(L)\n",
    "\t\tdL_dz = a - y  # d/dz of CE with sigmoid\n",
    "\t\tdL_dw = X.T @ dL_dz / len(X)\n",
    "\t\tdL_db = np.mean(dL_dz)\n",
    "\t\tw -= lr * dL_dw\n",
    "\t\tb -= lr * dL_db\n",
    "\t# plot data and decision boundary\n",
    "\tplt.figure(figsize=(10,4))\n",
    "\tplt.subplot(1,2,1)\n",
    "\tplt.scatter(X[y==1,0], X[y==1,1], c='tab:blue', label='y=1')\n",
    "\tplt.scatter(X[y==0,0], X[y==0,1], c='tab:orange', label='y=0')\n",
    "\txmin, xmax = X[:,0].min()-1, X[:,0].max()+1\n",
    "\tymin, ymax = X[:,1].min()-1, X[:,1].max()+1\n",
    "\txx, yy = np.meshgrid(np.linspace(xmin, xmax, 200), np.linspace(ymin, ymax, 200))\n",
    "\tzz = w[0] * xx + w[1] * yy + b\n",
    "\tpp = sigmoid(zz)\n",
    "\tplt.contour(xx, yy, zz, levels=[0.0], colors='k', linewidths=2)\n",
    "\tcf = plt.contourf(xx, yy, pp, levels=50, cmap='viridis', alpha=0.7)\n",
    "\tplt.colorbar(cf, label='p(y=1|x)')\n",
    "\tplt.legend()\n",
    "\tplt.title('Logistic neuron after training')\n",
    "\tplt.xlabel('x1'); plt.ylabel('x2'); plt.grid(alpha=0.2)\n",
    "\tplt.subplot(1,2,2)\n",
    "\tplt.plot(losses, label='loss')\n",
    "\tplt.xlabel('epoch'); plt.ylabel('binary cross-entropy')\n",
    "\tplt.title('Training loss')\n",
    "\tplt.grid(True, linestyle='--', alpha=0.5)\n",
    "\tplt.legend()\n",
    "\tplt.tight_layout()\n",
    "\tplt.show()\n",
    "\tprint(f\"Final params: w={w}, b={b:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: 3D surface of activation a(z) over (x1, x2)\n",
    "\n",
    "- **Why 3D?** Earlier, you saw 2D heatmaps of activation `a` over `(x1, x2)` for a fixed parameter set. The 3D surface shows the same thing with height as `a`, which makes slopes/flat regions easier to spot. Steep areas (large slope) mean larger gradients; flat plateaus mean small gradients (slow learning).\n",
    "- **Rotate the view:** Use the sliders to change azimuth/elevation and see the surface from different angles.\n",
    "- **Link to learning:** Where the surface is steep around your current `(x1, x2)`, gradient descent takes larger steps; where it is flat, steps are smaller.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b51c10a5583478cbba5e5ef649fdea6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='w1', max=2.0, min=-2.0), FloatSlider(value=1.0, desc…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D  # noqa: F401\n",
    "\n",
    "@interact(\n",
    "\tw1=FloatSlider(1.0, min=-2, max=2, step=0.1),\n",
    "\tw2=FloatSlider(1.0, min=-2, max=2, step=0.1),\n",
    "\tb=FloatSlider(0.0, min=-2, max=2, step=0.1),\n",
    "\tactivation=Dropdown(options=['sigmoid','tanh','relu','softplus'], value='sigmoid'),\n",
    "\tazim=IntSlider(45, min=0, max=360, step=5, description='azimuth'),\n",
    "\telev=IntSlider(25, min=0, max=90, step=5, description='elevation'),\n",
    ")\n",
    "def _activation_surface(w1, w2, b, activation, azim, elev):\n",
    "\tf, _ = _select_activation(activation)\n",
    "\tx = np.linspace(-3, 3, 60)\n",
    "\ty = np.linspace(-3, 3, 60)\n",
    "\tXX, YY = np.meshgrid(x, y)\n",
    "\tZ = w1 * XX + w2 * YY + b\n",
    "\tA = f(Z)\n",
    "\tfig = plt.figure(figsize=(7,5))\n",
    "\tax = fig.add_subplot(111, projection='3d')\n",
    "\tsurf = ax.plot_surface(XX, YY, A, cmap='viridis', linewidth=0, antialiased=True, alpha=0.85)\n",
    "\tfig.colorbar(surf, ax=ax, shrink=0.7, pad=0.1, label='activation a(z)')\n",
    "\tax.set_xlabel('x1'); ax.set_ylabel('x2'); ax.set_zlabel('activation')\n",
    "\tax.set_title(f'{activation} surface a(z)')\n",
    "\tax.view_init(elev=elev, azim=azim)\n",
    "\tplt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py3124t",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
